{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_McGwxU-SX7"
      },
      "source": [
        "Hi Everyone :)\n",
        "\n",
        " The below project will elaborate on how to create a deep learning model to classify images, where will take cat and dog as an example to prove its functionality.\n",
        "\n",
        "CNN will be used for this model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDz4JWOJ9ga1"
      },
      "source": [
        "# Starting by importing the libraries\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDbmQEw--B1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2545f67-7d91-4bf0-c820-97ffaf8491d7"
      },
      "source": [
        "#Uploading the training set for preprocessing\n",
        "#We used ImageDataGenerator to transform the images on a random basis \n",
        "\n",
        "train_datagen= ImageDataGenerator(rescale= 1./255,\n",
        "                                  shear_range = 0.2,\n",
        "                                  zoom_range=0.2,\n",
        "                                  horizontal_flip= True)\n",
        "\n",
        "training_set= train_datagen.flow_from_directory('/content/drive/MyDrive/Cats & Dogs Dataset/Training set',\n",
        "                                                target_size= (64,64),\n",
        "                                                batch_size= 32,\n",
        "                                                class_mode= 'binary')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8005 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeupuj2MMJca"
      },
      "source": [
        "1- Rescale: rescale is a value by which we will multiply the data before any other processing. Our original images consist in RGB coefficients in the 0-255\n",
        "\n",
        "2- Shear range: means that the image will be distorted along an axis, mostly to create or rectify the perception angles. It's usually used to augment images so that computers can see how humans see things from different angles\n",
        "\n",
        "3- Zoom_range is for randomly zooming inside pictures.\n",
        "\n",
        "4- Horizontal_flip is for randomly flipping half of the images horizontally \n",
        "\n",
        "5- Target_size: tuple of integers (height, width), default: (256, 256)\n",
        "\n",
        "6- Batch_size denotes the subset size of your training sample\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mmEwWLSAoYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c057691e-213a-4b06-e934-66dd1419352c"
      },
      "source": [
        "#preprocessing the Test set on the same way of Traninig set\n",
        "\n",
        "test_datagen= ImageDataGenerator(rescale= 1./255)\n",
        "test_set= test_datagen.flow_from_directory('/content/drive/MyDrive/Cats & Dogs Dataset/Testing set',\n",
        "                                            target_size= (64,64),\n",
        "                                            batch_size= 32,\n",
        "                                            class_mode= 'binary')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2023 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH9JXxTOR9Zf"
      },
      "source": [
        "1- kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "\n",
        "2-ReLu is a type of non-linear activation function. It helps the model understand which neurons are activating.\n",
        "\n",
        "3- Pooling refers to calculating the aggregate statistic over the regions of the convolved feature space.\n",
        "\n",
        "4- strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution.\n",
        "\n",
        "5- Dropout: The primary purpose of dropout is to minimize the effect of overfitting within a trained network.\n",
        "Dropout technique works by randomly reducing the number of interconnecting neurons within a neural network. At every training step, each neuron has a chance of being left out, or rather, dropped out of the collated contribution from connected neurons.\n",
        "\n",
        "6- Units: The amount of neurons inside the layer.\n",
        "\n",
        "7- Dense:This is a simple fully connected neural network layer.\n",
        "This layer produces the output of the following function: activation((inputs x weights)+bias) where activation refers to the activation function passed to the layer, which is None by default.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is time for designing our CNN model "
      ],
      "metadata": {
        "id": "e27fq_IsMpuF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snceBh4pBki2"
      },
      "source": [
        "#Initilising the CNN\n",
        "cnn= tf.keras.models.Sequential()\n",
        "\n",
        "#Step 1 - Convolution\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64,3]))\n",
        "\n",
        "# Step two: Pooling\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "#Adding the second convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "#Adding the Third convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=128,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "#Adding The forth convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=128,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "#Implementing Dropout\n",
        "cnn.add(tf.keras.layers.Dropout(0.5)) \n",
        "\n",
        "# Step 3 - Flattening\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "#step 4 - Full Connection\n",
        "cnn.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
        "\n",
        "# Step 5 - Output Layer\n",
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxgQkGgFWzF5"
      },
      "source": [
        "1- The Adam optimization algorithm is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSoRy-ZFOkPo"
      },
      "source": [
        "# Compiling the CNN\n",
        "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqA70RFlmSpK",
        "outputId": "9f661003-fd75-4c5c-fcf6-ea67390dfda4"
      },
      "source": [
        "img, label = training_set[100]\n",
        "print(img.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe9O3jI4ZU-J"
      },
      "source": [
        "Param= ((kernal X Kernal X Channels) + 1)X filters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GldbRH7g5_qu",
        "outputId": "94727368-80d0-42bd-8922-3ab30c66eb6c"
      },
      "source": [
        "# look at the defined model\n",
        "cnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 62, 62, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 29, 29, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 504,001\n",
            "Trainable params: 504,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_ciY86dT_Qm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea7c214-ad1a-4349-f45b-9054425154a7"
      },
      "source": [
        "# Training the CNN\n",
        "cnn.fit(x= training_set, validation_data=test_set,epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.6906 - accuracy: 0.5212 - val_loss: 0.6797 - val_accuracy: 0.5625\n",
            "Epoch 2/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.6721 - accuracy: 0.5876 - val_loss: 0.6638 - val_accuracy: 0.6362\n",
            "Epoch 3/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.6531 - accuracy: 0.6194 - val_loss: 0.6407 - val_accuracy: 0.6377\n",
            "Epoch 4/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.6252 - accuracy: 0.6491 - val_loss: 0.5883 - val_accuracy: 0.6812\n",
            "Epoch 5/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.5789 - accuracy: 0.6957 - val_loss: 0.5226 - val_accuracy: 0.7375\n",
            "Epoch 6/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.5418 - accuracy: 0.7288 - val_loss: 0.5016 - val_accuracy: 0.7528\n",
            "Epoch 7/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.5073 - accuracy: 0.7519 - val_loss: 0.4703 - val_accuracy: 0.7766\n",
            "Epoch 8/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.4858 - accuracy: 0.7681 - val_loss: 0.5166 - val_accuracy: 0.7316\n",
            "Epoch 9/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.4684 - accuracy: 0.7736 - val_loss: 0.4226 - val_accuracy: 0.8087\n",
            "Epoch 10/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.4466 - accuracy: 0.7885 - val_loss: 0.4179 - val_accuracy: 0.8122\n",
            "Epoch 11/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.4305 - accuracy: 0.7989 - val_loss: 0.4501 - val_accuracy: 0.7657\n",
            "Epoch 12/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.4099 - accuracy: 0.8116 - val_loss: 0.4108 - val_accuracy: 0.8245\n",
            "Epoch 13/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.4081 - accuracy: 0.8130 - val_loss: 0.4105 - val_accuracy: 0.8146\n",
            "Epoch 14/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.3884 - accuracy: 0.8241 - val_loss: 0.4157 - val_accuracy: 0.8151\n",
            "Epoch 15/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.3778 - accuracy: 0.8260 - val_loss: 0.3753 - val_accuracy: 0.8319\n",
            "Epoch 16/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.3758 - accuracy: 0.8269 - val_loss: 0.3859 - val_accuracy: 0.8166\n",
            "Epoch 17/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.3556 - accuracy: 0.8440 - val_loss: 0.3900 - val_accuracy: 0.8216\n",
            "Epoch 18/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.3367 - accuracy: 0.8487 - val_loss: 0.3914 - val_accuracy: 0.8265\n",
            "Epoch 19/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.3387 - accuracy: 0.8510 - val_loss: 0.3548 - val_accuracy: 0.8369\n",
            "Epoch 20/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.3337 - accuracy: 0.8491 - val_loss: 0.3501 - val_accuracy: 0.8463\n",
            "Epoch 21/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.3229 - accuracy: 0.8526 - val_loss: 0.3315 - val_accuracy: 0.8576\n",
            "Epoch 22/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.3105 - accuracy: 0.8641 - val_loss: 0.3652 - val_accuracy: 0.8329\n",
            "Epoch 23/100\n",
            "251/251 [==============================] - 91s 361ms/step - loss: 0.3037 - accuracy: 0.8653 - val_loss: 0.3621 - val_accuracy: 0.8428\n",
            "Epoch 24/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.2995 - accuracy: 0.8705 - val_loss: 0.3286 - val_accuracy: 0.8591\n",
            "Epoch 25/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.2847 - accuracy: 0.8743 - val_loss: 0.3489 - val_accuracy: 0.8527\n",
            "Epoch 26/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.2886 - accuracy: 0.8690 - val_loss: 0.3607 - val_accuracy: 0.8478\n",
            "Epoch 27/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.2894 - accuracy: 0.8736 - val_loss: 0.3429 - val_accuracy: 0.8542\n",
            "Epoch 28/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.2857 - accuracy: 0.8782 - val_loss: 0.3529 - val_accuracy: 0.8522\n",
            "Epoch 29/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.2828 - accuracy: 0.8797 - val_loss: 0.3573 - val_accuracy: 0.8438\n",
            "Epoch 30/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.2646 - accuracy: 0.8844 - val_loss: 0.3277 - val_accuracy: 0.8581\n",
            "Epoch 31/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.2764 - accuracy: 0.8858 - val_loss: 0.3202 - val_accuracy: 0.8606\n",
            "Epoch 32/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.2615 - accuracy: 0.8874 - val_loss: 0.3596 - val_accuracy: 0.8468\n",
            "Epoch 33/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.2593 - accuracy: 0.8897 - val_loss: 0.3216 - val_accuracy: 0.8705\n",
            "Epoch 34/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.2452 - accuracy: 0.8994 - val_loss: 0.3035 - val_accuracy: 0.8700\n",
            "Epoch 35/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.2478 - accuracy: 0.8951 - val_loss: 0.3335 - val_accuracy: 0.8606\n",
            "Epoch 36/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.2558 - accuracy: 0.8929 - val_loss: 0.2984 - val_accuracy: 0.8665\n",
            "Epoch 37/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.2360 - accuracy: 0.9011 - val_loss: 0.3463 - val_accuracy: 0.8631\n",
            "Epoch 38/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.2424 - accuracy: 0.8969 - val_loss: 0.3589 - val_accuracy: 0.8463\n",
            "Epoch 39/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.2312 - accuracy: 0.9036 - val_loss: 0.3202 - val_accuracy: 0.8651\n",
            "Epoch 40/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.2276 - accuracy: 0.9051 - val_loss: 0.3337 - val_accuracy: 0.8552\n",
            "Epoch 41/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.2260 - accuracy: 0.9054 - val_loss: 0.3161 - val_accuracy: 0.8685\n",
            "Epoch 42/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.2285 - accuracy: 0.9042 - val_loss: 0.2947 - val_accuracy: 0.8779\n",
            "Epoch 43/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.2195 - accuracy: 0.9084 - val_loss: 0.3377 - val_accuracy: 0.8616\n",
            "Epoch 44/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.2177 - accuracy: 0.9092 - val_loss: 0.3199 - val_accuracy: 0.8571\n",
            "Epoch 45/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.2185 - accuracy: 0.9064 - val_loss: 0.3028 - val_accuracy: 0.8730\n",
            "Epoch 46/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.2049 - accuracy: 0.9127 - val_loss: 0.3259 - val_accuracy: 0.8749\n",
            "Epoch 47/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.2122 - accuracy: 0.9114 - val_loss: 0.3191 - val_accuracy: 0.8784\n",
            "Epoch 48/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.1968 - accuracy: 0.9210 - val_loss: 0.3758 - val_accuracy: 0.8586\n",
            "Epoch 49/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.2008 - accuracy: 0.9183 - val_loss: 0.3203 - val_accuracy: 0.8685\n",
            "Epoch 50/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.2044 - accuracy: 0.9121 - val_loss: 0.3532 - val_accuracy: 0.8586\n",
            "Epoch 51/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1928 - accuracy: 0.9176 - val_loss: 0.3326 - val_accuracy: 0.8651\n",
            "Epoch 52/100\n",
            "251/251 [==============================] - 92s 366ms/step - loss: 0.1976 - accuracy: 0.9184 - val_loss: 0.3273 - val_accuracy: 0.8700\n",
            "Epoch 53/100\n",
            "251/251 [==============================] - 92s 367ms/step - loss: 0.1893 - accuracy: 0.9215 - val_loss: 0.3553 - val_accuracy: 0.8636\n",
            "Epoch 54/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.1877 - accuracy: 0.9243 - val_loss: 0.3256 - val_accuracy: 0.8680\n",
            "Epoch 55/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1884 - accuracy: 0.9204 - val_loss: 0.3498 - val_accuracy: 0.8784\n",
            "Epoch 56/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.1862 - accuracy: 0.9227 - val_loss: 0.3513 - val_accuracy: 0.8596\n",
            "Epoch 57/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.1894 - accuracy: 0.9202 - val_loss: 0.4297 - val_accuracy: 0.8443\n",
            "Epoch 58/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.1901 - accuracy: 0.9239 - val_loss: 0.3525 - val_accuracy: 0.8651\n",
            "Epoch 59/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1804 - accuracy: 0.9264 - val_loss: 0.3176 - val_accuracy: 0.8710\n",
            "Epoch 60/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.1867 - accuracy: 0.9225 - val_loss: 0.3609 - val_accuracy: 0.8581\n",
            "Epoch 61/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.1796 - accuracy: 0.9283 - val_loss: 0.3223 - val_accuracy: 0.8749\n",
            "Epoch 62/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.1772 - accuracy: 0.9265 - val_loss: 0.3669 - val_accuracy: 0.8680\n",
            "Epoch 63/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1794 - accuracy: 0.9257 - val_loss: 0.2979 - val_accuracy: 0.8779\n",
            "Epoch 64/100\n",
            "251/251 [==============================] - 92s 366ms/step - loss: 0.1727 - accuracy: 0.9299 - val_loss: 0.3590 - val_accuracy: 0.8814\n",
            "Epoch 65/100\n",
            "251/251 [==============================] - 96s 381ms/step - loss: 0.1759 - accuracy: 0.9268 - val_loss: 0.3084 - val_accuracy: 0.8838\n",
            "Epoch 66/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.1602 - accuracy: 0.9349 - val_loss: 0.3331 - val_accuracy: 0.8814\n",
            "Epoch 67/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.1597 - accuracy: 0.9328 - val_loss: 0.3903 - val_accuracy: 0.8690\n",
            "Epoch 68/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1630 - accuracy: 0.9333 - val_loss: 0.3424 - val_accuracy: 0.8700\n",
            "Epoch 69/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1660 - accuracy: 0.9334 - val_loss: 0.3350 - val_accuracy: 0.8730\n",
            "Epoch 70/100\n",
            "251/251 [==============================] - 92s 367ms/step - loss: 0.1601 - accuracy: 0.9368 - val_loss: 0.3626 - val_accuracy: 0.8660\n",
            "Epoch 71/100\n",
            "251/251 [==============================] - 92s 367ms/step - loss: 0.1619 - accuracy: 0.9338 - val_loss: 0.3471 - val_accuracy: 0.8700\n",
            "Epoch 72/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1634 - accuracy: 0.9324 - val_loss: 0.3508 - val_accuracy: 0.8784\n",
            "Epoch 73/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1572 - accuracy: 0.9360 - val_loss: 0.3485 - val_accuracy: 0.8665\n",
            "Epoch 74/100\n",
            "251/251 [==============================] - 92s 366ms/step - loss: 0.1578 - accuracy: 0.9342 - val_loss: 0.3522 - val_accuracy: 0.8759\n",
            "Epoch 75/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1571 - accuracy: 0.9353 - val_loss: 0.3214 - val_accuracy: 0.8715\n",
            "Epoch 76/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1504 - accuracy: 0.9368 - val_loss: 0.3482 - val_accuracy: 0.8814\n",
            "Epoch 77/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.1530 - accuracy: 0.9362 - val_loss: 0.3326 - val_accuracy: 0.8838\n",
            "Epoch 78/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.1523 - accuracy: 0.9382 - val_loss: 0.3397 - val_accuracy: 0.8660\n",
            "Epoch 79/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.1501 - accuracy: 0.9377 - val_loss: 0.4027 - val_accuracy: 0.8606\n",
            "Epoch 80/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.1455 - accuracy: 0.9410 - val_loss: 0.3429 - val_accuracy: 0.8705\n",
            "Epoch 81/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.1528 - accuracy: 0.9385 - val_loss: 0.3094 - val_accuracy: 0.8754\n",
            "Epoch 82/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.1364 - accuracy: 0.9435 - val_loss: 0.3307 - val_accuracy: 0.8784\n",
            "Epoch 83/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.1548 - accuracy: 0.9379 - val_loss: 0.2907 - val_accuracy: 0.8824\n",
            "Epoch 84/100\n",
            "251/251 [==============================] - 91s 362ms/step - loss: 0.1418 - accuracy: 0.9404 - val_loss: 0.2957 - val_accuracy: 0.8774\n",
            "Epoch 85/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1383 - accuracy: 0.9435 - val_loss: 0.3555 - val_accuracy: 0.8809\n",
            "Epoch 86/100\n",
            "251/251 [==============================] - 92s 366ms/step - loss: 0.1471 - accuracy: 0.9402 - val_loss: 0.3877 - val_accuracy: 0.8646\n",
            "Epoch 87/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.1334 - accuracy: 0.9492 - val_loss: 0.3648 - val_accuracy: 0.8725\n",
            "Epoch 88/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1365 - accuracy: 0.9450 - val_loss: 0.3043 - val_accuracy: 0.8735\n",
            "Epoch 89/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1316 - accuracy: 0.9457 - val_loss: 0.3868 - val_accuracy: 0.8715\n",
            "Epoch 90/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.1409 - accuracy: 0.9418 - val_loss: 0.3025 - val_accuracy: 0.8665\n",
            "Epoch 91/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1353 - accuracy: 0.9454 - val_loss: 0.3280 - val_accuracy: 0.8715\n",
            "Epoch 92/100\n",
            "251/251 [==============================] - 91s 363ms/step - loss: 0.1321 - accuracy: 0.9460 - val_loss: 0.3497 - val_accuracy: 0.8789\n",
            "Epoch 93/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.1362 - accuracy: 0.9425 - val_loss: 0.3758 - val_accuracy: 0.8789\n",
            "Epoch 94/100\n",
            "251/251 [==============================] - 91s 364ms/step - loss: 0.1310 - accuracy: 0.9495 - val_loss: 0.3802 - val_accuracy: 0.8799\n",
            "Epoch 95/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.1368 - accuracy: 0.9455 - val_loss: 0.3482 - val_accuracy: 0.8665\n",
            "Epoch 96/100\n",
            "251/251 [==============================] - 92s 366ms/step - loss: 0.1279 - accuracy: 0.9480 - val_loss: 0.3803 - val_accuracy: 0.8725\n",
            "Epoch 97/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.1388 - accuracy: 0.9444 - val_loss: 0.3793 - val_accuracy: 0.8695\n",
            "Epoch 98/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.1237 - accuracy: 0.9533 - val_loss: 0.4274 - val_accuracy: 0.8611\n",
            "Epoch 99/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.1233 - accuracy: 0.9543 - val_loss: 0.3661 - val_accuracy: 0.8660\n",
            "Epoch 100/100\n",
            "251/251 [==============================] - 92s 365ms/step - loss: 0.1248 - accuracy: 0.9498 - val_loss: 0.3696 - val_accuracy: 0.8710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3de779afd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGxMqphrofdE",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "493400c1-81ec-460b-c413-d4c4fa94228b"
      },
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(64, 64))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = cnn.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a dog\")\n",
        "  else:\n",
        "    print(fn + \" is a cat\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1c8e369e-6859-470d-a7b0-19b01082bd94\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1c8e369e-6859-470d-a7b0-19b01082bd94\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving cat_Test3.jpg to cat_Test3 (1).jpg\n",
            "[0.]\n",
            "cat_Test3.jpg is a cat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EgrJgWXWQud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee817ba1-93a0-470d-ae3d-2a2cefebdb99"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image= image.load_img('/content/Cat2.jpeg', target_size=(64,64))\n",
        "test_image= image.img_to_array(test_image)\n",
        "test_image= np.expand_dims(test_image,axis=0)\n",
        "result= cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0]==1:\n",
        "  prediction='Dog'\n",
        "else:\n",
        "  prediction='Cat'\n",
        "print(prediction)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dog\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}